{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6SAoewEZSZk",
    "outputId": "da3b709a-25a8-42d9-c013-0223615af1e9"
   },
   "outputs": [],
   "source": [
    "!pip install torchtext==0.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUsNgYkTURxW",
    "outputId": "0fea8915-b847-4a08-d1f2-ac2d83711901"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download uk_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ND7lBihVbnRf",
    "outputId": "611c3ff1-d8e1-444c-86e6-0acf1c5992f5"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qev-16RzbzOJ",
    "outputId": "897ca901-244b-415e-a70f-f95aac2b0eb0"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds2016 = load_dataset(\"turuta/Multi30k-uk\", \"flickr_2016\")\n",
    "ds2017 = load_dataset(\"turuta/Multi30k-uk\", \"flickr_2017\")\n",
    "ds2018 = load_dataset(\"turuta/Multi30k-uk\", \"flickr_2018\")\n",
    "multi = load_dataset(\"turuta/Multi30k-uk\", \"multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDcQ8DiRb5jq",
    "outputId": "877892a5-7418-41ce-eb7b-9159fd0a2f94"
   },
   "outputs": [],
   "source": [
    "print(f'ds2016 {ds2016}')\n",
    "print(f\"ds2017 {ds2017}\")\n",
    "print(f\"ds2018 {ds2018}\")\n",
    "print(f\"multi {multi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bF5Mirk4b77k",
    "outputId": "57fc9965-f468-41b9-ab39-14c926872df7"
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "\n",
    "combined_train = concatenate_datasets([\n",
    "    ds2016[\"train\"],\n",
    "    ds2017[\"train\"],\n",
    "    ds2018[\"train\"],\n",
    "    multi[\"train\"]\n",
    "])\n",
    "\n",
    "train_val_indices, test_indices = sklearn_train_test_split(\n",
    "    list(range(len(combined_train))),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_indices, val_indices = sklearn_train_test_split(\n",
    "    train_val_indices,\n",
    "    test_size=0.1,  # 10% від 80% = 8% від всього\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = combined_train.select(train_indices)\n",
    "val_dataset = combined_train.select(val_indices)\n",
    "test_dataset = combined_train.select(test_indices)\n",
    "\n",
    "combined_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyi1tNf1VHaF",
    "outputId": "55bfe4e6-1f0c-49fd-ba4b-50a69d42a134"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "uk_nlp = spacy.load(\"uk_core_news_sm\")\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "string = \"Hi, what are you doing\"\n",
    "\n",
    "print([token.text for token in en_nlp.tokenizer(string)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpy4_lylVUJd"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [\n",
    "        token for token in tokens\n",
    "        if token.strip() and token.strip() not in {\",\", \".\", \"!\", \"?\", \":\", \";\", \"\\xa0\", \" \"}\n",
    "    ]\n",
    "\n",
    "def tokenize_example(example, uk_nlp, en_nlp, max_length, lower, sos_token, eos_token):\n",
    "    uk_tokens = [token.text for token in uk_nlp.tokenizer(example[\"uk\"])][:max_length]\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    if lower:\n",
    "        uk_tokens = [token.lower() for token in uk_tokens]\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "\n",
    "    uk_tokens = clean_tokens(uk_tokens)\n",
    "    en_tokens = clean_tokens(en_tokens)\n",
    "\n",
    "    uk_tokens = [sos_token] + uk_tokens + [eos_token]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    return {\"uk_tokens\": uk_tokens, \"en_tokens\": en_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "9c6d405d970c45e5ad7f315734b9ab97",
      "3d3eba23f6f54a1d80b33f0512f71cef",
      "eea6b8ecc9a444a88732564c1a51d52c",
      "09671ec5e32c4f1ea9a1659815595fbc",
      "5c9c70f8cdfd4a02b029191203a7ea43",
      "b2ea55403eb64261a84a7d6dad9f4f90",
      "78d8794a1ce3437fb13f5c02c981a451",
      "3c8d1df1e5be4436bb9e155438e850c2",
      "19374b3f84b844c79299c2caaabe08ac",
      "d77282b2b219460796212983b1513091",
      "ac0d271f89214a6ca83d9e6118e70549",
      "dd69c98e3fcc4787abf915ba82277122",
      "60fc57571fd74c9089486ad2e51a8917",
      "05c41f6858fa4bcaab1ff12db959ce66",
      "197faf94c93e4180ab2478f057455b56",
      "21065bc6596c4bd1804f28e62050f652",
      "47c72a500da943f59ab300c4555f7dd9",
      "6afb88c36abd4413981fb6159fd7e8e3",
      "7686f7415f6e44a280bd3f013aee6455",
      "115b01a4eb5a4c519b66c7544e9de6c2",
      "a56da4aba58649319b278dd621b2baac",
      "38e17d4c13244f5f9bad30dded43205d",
      "e732a318884f48119c8a4134faa6ac0c",
      "86e6fdca5b8444a994ac93b9034e75cb",
      "233c8b39c3b84c30a7c834896f85f701",
      "cf26c7e1b91049ffae674b2577d8279d",
      "80e62f38ab514b05959a598bf1b71fd7",
      "d36b573d6a4d4da29825f7030196c6a5",
      "1209c26ca15f43eeba3e2654ae7b308b",
      "acf3a89e00004e28acf9dfdcaaac78c3",
      "a20bbcde2d014532ac2fa4984ccceba2",
      "a82407800fd843a4b7b8fc0533e9f246",
      "9e1d3810734b4d258fb7413bf060152f"
     ]
    },
    "id": "f3nP6qFVVYDF",
    "outputId": "c01f06c1-058e-4351-b981-642de78c0b09"
   },
   "outputs": [],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"uk_nlp\": uk_nlp,\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = val_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAqaU6qmVhHO",
    "outputId": "565f161e-00d2-4af8-b7d8-4763e8aa584b"
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBHNDdaZVk3k"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "uk_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"uk_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpblorV4Vo-U",
    "outputId": "fdb335a1-5dee-42c0-c6a1-d9118cb2d01e"
   },
   "outputs": [],
   "source": [
    "uk_vocab.get_itos()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Redi3dwfVtEU",
    "outputId": "4c96314a-22dd-4b8c-91a1-5aa6a67ea20b"
   },
   "outputs": [],
   "source": [
    "en_vocab.get_itos()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzmDLyx9Vtrd",
    "outputId": "938280a8-1164-424f-bc05-ee610a56b319"
   },
   "outputs": [],
   "source": [
    "len(uk_vocab), len(en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShKs9j6dVxAN"
   },
   "outputs": [],
   "source": [
    "assert uk_vocab[unk_token] == en_vocab[unk_token]\n",
    "assert uk_vocab[pad_token] == en_vocab[pad_token]\n",
    "\n",
    "unk_index = uk_vocab[unk_token]\n",
    "pad_index = uk_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpVdyYaZV6ju"
   },
   "outputs": [],
   "source": [
    "uk_vocab.set_default_index(unk_index)\n",
    "en_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_aVUfQp7V8iz",
    "outputId": "3245f1c5-7365-48f5-af68-fac1e9c29896"
   },
   "outputs": [],
   "source": [
    "tokens = ['a',\n",
    " 'in',\n",
    " 'the',\n",
    " 'on',\n",
    " 'man',\n",
    " 'is',\n",
    " 'and',\n",
    " 'of']\n",
    "en_vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPqz9rT0WA8d",
    "outputId": "210b8cce-c0e7-41b0-8bfa-354eb3f39c49"
   },
   "outputs": [],
   "source": [
    "en_vocab.lookup_tokens(en_vocab.lookup_indices(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvM1_F0UWBod"
   },
   "outputs": [],
   "source": [
    "def numericalize_example(example, uk_vocab, en_vocab):\n",
    "    uk_ids = uk_vocab.lookup_indices(example[\"uk_tokens\"])\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    return {\"uk_ids\": uk_ids, \"en_ids\": en_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "67d156056c374280ba7a45a9a7256c87",
      "48a4c8ae772545619ab3fe3bb2d11f73",
      "662dc47b74e64ca1a70907e2082a810c",
      "202e5276645c4c3f9b89c1210c450acb",
      "dbdf6e5332224a29b90f01383f1ab4fe",
      "016be18018b049428edb49ca53799b47",
      "4824ab5c403640969c0176aa364bf80d",
      "235c9697353d4a1692d31f7d6a0e395d",
      "d819535bc738482d9641c0858650cbf1",
      "c4ad87b258bd48e1b0d1d23b7902c902",
      "602ea2314675444c8ff5ab809d53b9dc",
      "4b31d52a59b5499ea31db4d624a663b7",
      "58e2d47f7a6b4eaebb41395c21bf57f0",
      "e379e5734d2e4ec692ed5024b8e0f06f",
      "aba6213a019f46e694849bc3a2ff3675",
      "3e26ad6f7a604baabdac6ce7b3e19120",
      "2006aa71da054ed69f54bbc6c395e001",
      "29608d5340b2469295b8afc061486e66",
      "d21a249ab82d40a29a9d643cc7a86ca5",
      "4f4ca769d4c84f59b0341feae604b358",
      "01c47e85e4ca4094bd9c817f32f8a03a",
      "f6461798f4b04a2d897f233d5b2c914d",
      "73cbdd3de69f433fa8dd8d680fdee5d2",
      "c75dd81b97bc4302bb93a1f87c288ec9",
      "3a15022704ad4029ac4e9d9ac480a722",
      "fd0704a7c9b145268e3ba701dcb40e8e",
      "1dc861b4c0b346dbb1c886765360ec1a",
      "66ab360ef0c349dfbac8160064fb0807",
      "a3a6c5a1d3514e33b29df97f560707a5",
      "76a663a5c8884da994758b7f8e1feff7",
      "e8a8dfa3dbdf4449b8a35fb053cae6f5",
      "056122af8fe841ac9e74fe33cf82aa19",
      "7eeaf839d70146e3a94e302db8118ea4"
     ]
    },
    "id": "78oWid4JWEAl",
    "outputId": "39c0947d-8193-49b7-bbb7-525d59df1b04"
   },
   "outputs": [],
   "source": [
    "fn_kwargs = {\"uk_vocab\": uk_vocab, \"en_vocab\": en_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqif728-WOY9",
    "outputId": "190eff0b-ea95-41c9-e8b9-721803c96186"
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vbtprly-WSEL",
    "outputId": "78e194fa-b7d6-443f-c444-ec58a6601525"
   },
   "outputs": [],
   "source": [
    "uk_vocab.lookup_tokens(train_data[0][\"uk_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsKV1GSjWUcc"
   },
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"uk_ids\", \"en_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9cFibeBWXur",
    "outputId": "e43876b7-c3a4-41ce-ef52-ea91c9263c55"
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dy-T0t0WZ_s"
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_uk_ids = [example[\"uk_ids\"] for example in batch]\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_uk_ids = nn.utils.rnn.pad_sequence(batch_uk_ids, padding_value=pad_index, batch_first=True)\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index, batch_first=True)\n",
    "        batch = {\n",
    "            \"uk_ids\": batch_uk_ids,\n",
    "            \"en_ids\": batch_en_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPacClvBbpyp",
    "outputId": "24b2291c-828d-4383-a0ab-8517c5db8b90"
   },
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixRA-BD0WcqD"
   },
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pt-r4myWfJE"
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyXKij8nfsv4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Embedding the input sequence\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim, dropout=0.1):\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_len, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)  # (1, seq_len)\n",
    "        pos_embed = self.pos_embedding(positions)  # (1, seq_len, d_model)\n",
    "        x = x + pos_embed\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Self-attention layer\n",
    "class SelfAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        key_dim = key.size(-1)\n",
    "        attn = torch.matmul(query / np.sqrt(key_dim), key.transpose(2, 3))\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = self.dropout(torch.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, value)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Multi-head attention layer\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert embedding_dim % num_heads == 0, \"embedding_dim must be divisible by num_heads\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.self_attention = SelfAttention(dropout)\n",
    "        # The number of heads\n",
    "        self.num_heads = num_heads\n",
    "        # The dimension of each head\n",
    "        self.dim_per_head = embedding_dim // num_heads\n",
    "        # The linear projections\n",
    "        self.query_projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.key_projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.value_projection = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "      batch_size = query.size(0)\n",
    "\n",
    "      # Apply linear projections\n",
    "      query = self.query_projection(query)\n",
    "      key = self.key_projection(key)\n",
    "      value = self.value_projection(value)\n",
    "\n",
    "      # Reshape for multi-head attention\n",
    "      query = query.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "      key = key.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "      value = value.view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
    "\n",
    "      # Calculate attention\n",
    "      scores = self.self_attention(query, key, value, mask)\n",
    "\n",
    "      # Reshape back\n",
    "      output = scores.transpose(1, 2).contiguous().view(batch_size, -1, self.embedding_dim)\n",
    "\n",
    "      # Final linear projection\n",
    "      output = self.out(output)\n",
    "      return output\n",
    "\n",
    "# Norm layer\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Norm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# Transformer encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, ff_dim=2048, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embedding_dim)\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm1 = Norm(embedding_dim)\n",
    "        self.norm2 = Norm(embedding_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "        # Add and Muti-head attention\n",
    "        x = x + self.dropout1(self.self_attention(x2, x2, x2, mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.feed_forward(x2))\n",
    "        return x\n",
    "\n",
    "# Transformer decoder layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, ff_dim=2048, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
    "        self.encoder_attention = MultiHeadAttention(embedding_dim, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embedding_dim)\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.norm1 = Norm(embedding_dim)\n",
    "        self.norm2 = Norm(embedding_dim)\n",
    "        self.norm3 = Norm(embedding_dim)\n",
    "\n",
    "    def forward(self, x, memory, source_mask, target_mask):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout1(self.self_attention(x2, x2, x2, target_mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.encoder_attention(x2, memory, memory, source_mask))\n",
    "        x2 = self.norm3(x)\n",
    "        x = x + self.dropout3(self.feed_forward(x2))\n",
    "        return x\n",
    "\n",
    "# Encoder transformer\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len, num_heads, num_layers, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.layers = nn.ModuleList([EncoderLayer(embedding_dim, num_heads, 2048, dropout) for _ in range(num_layers)])\n",
    "        self.norm = Norm(embedding_dim)\n",
    "        # self.position_embedding = PositionalEncoder(embedding_dim, max_seq_len, dropout)\n",
    "        self.position_embedding = LearnablePositionalEncoding(max_seq_len, embedding_dim)  # використовуємо навчуваний енкодинг\n",
    "\n",
    "    def forward(self, source, source_mask):\n",
    "        # Embed the source\n",
    "        x = self.embedding(source)\n",
    "        # Add the position embeddings\n",
    "        x = self.position_embedding(x)\n",
    "        # Propagate through the layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, source_mask)\n",
    "        # Normalize\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# Decoder transformer\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_seq_len,num_heads, num_layers, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.layers = nn.ModuleList([DecoderLayer(embedding_dim, num_heads, 2048, dropout) for _ in range(num_layers)])\n",
    "        self.norm = Norm(embedding_dim)\n",
    "        # self.position_embedding = PositionalEncoder(embedding_dim, max_seq_len, dropout)\n",
    "        self.position_embedding = LearnablePositionalEncoding(max_seq_len, embedding_dim)\n",
    "\n",
    "    def forward(self, target, memory, source_mask, target_mask):\n",
    "        # Embed the source\n",
    "        x = self.embedding(target)\n",
    "        # Add the position embeddings\n",
    "        x = self.position_embedding(x)\n",
    "        # Propagate through the layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, source_mask, target_mask)\n",
    "        # Normalize\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# Transformers\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, source_max_seq_len, target_max_seq_len, embedding_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.source_max_seq_len = source_max_seq_len\n",
    "        self.target_max_seq_len = target_max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = Encoder(source_vocab_size, embedding_dim, source_max_seq_len, num_heads, num_layers, dropout)\n",
    "        self.decoder = Decoder(target_vocab_size, embedding_dim, target_max_seq_len, num_heads, num_layers, dropout)\n",
    "        self.final_linear = nn.Linear(embedding_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        # Encoder forward pass\n",
    "        memory = self.encoder(source, source_mask)\n",
    "        # Decoder forward pass\n",
    "        output = self.decoder(target, memory, source_mask, target_mask)\n",
    "        # Final linear layer\n",
    "        output = self.dropout(output)\n",
    "        output = self.final_linear(output)\n",
    "        return output\n",
    "\n",
    "    def make_source_mask(self, source_ids, source_pad_id):\n",
    "        return (source_ids != source_pad_id).unsqueeze(-2)\n",
    "\n",
    "    def make_target_mask(self, target_ids):\n",
    "        batch_size, len_target = target_ids.size()\n",
    "        subsequent_mask = (1 - torch.triu(torch.ones((1, len_target, len_target), device=target_ids.device), diagonal=1)).bool()\n",
    "        return subsequent_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMivatf9j_Hk"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Transformer(\n",
    "    source_vocab_size=len(uk_vocab),\n",
    "    target_vocab_size=len(en_vocab),\n",
    "    embedding_dim=512,\n",
    "    source_max_seq_len=256,\n",
    "    target_max_seq_len=256,\n",
    "    num_layers=6,\n",
    "    num_heads=8,\n",
    "    dropout=0.2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KH-GKN_nicOU"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Qmb-PUzjz8H"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klHxqoLUhvgu"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, train_loader, optim, epoch, n_epochs, source_pad_id, target_pad_id, device):\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training epoch {epoch+1}/{n_epochs}\")\n",
    "    for i, batch in bar:\n",
    "        source, target = batch[\"uk_ids\"].to(device), batch[\"en_ids\"].to(device)\n",
    "        target_input = target[:, :-1]\n",
    "        source_mask, target_mask = model.make_source_mask(source, source_pad_id), model.make_target_mask(target_input)\n",
    "        preds = model(source, target_input, source_mask, target_mask)\n",
    "        optim.zero_grad()\n",
    "        gold = target[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(preds.view(-1, preds.size(-1)), gold.view(-1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss.append(loss.item())\n",
    "        bar.set_postfix(loss=total_loss[-1])\n",
    "\n",
    "    train_loss = sum(total_loss) / len(total_loss)\n",
    "    return train_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXEtsXL6sOwT"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_epoch(model, val_loader, epoch, n_epochs, source_pad_id, target_pad_id, device):\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Validating epoch {epoch+1}/{n_epochs}\")\n",
    "\n",
    "    for i, batch in bar:\n",
    "        source, target = batch[\"uk_ids\"].to(device), batch[\"en_ids\"].to(device)\n",
    "        target_input = target[:, :-1]\n",
    "        source_mask = model.make_source_mask(source, source_pad_id)\n",
    "        target_mask = model.make_target_mask(target_input)\n",
    "\n",
    "        preds = model(source, target_input, source_mask, target_mask)\n",
    "        gold = target[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(preds.view(-1, preds.size(-1)), gold.view(-1))\n",
    "        total_loss.append(loss.item())\n",
    "        bar.set_postfix(loss=total_loss[-1])\n",
    "\n",
    "    val_loss = sum(total_loss) / len(total_loss)\n",
    "    return val_loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4d-nF5idjnxx",
    "outputId": "ecffe007-26cb-4b98-c7c2-beb6adfa3b82"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_epochs = 45\n",
    "patience = 3 \n",
    "best_valid_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_losses = train_epoch(\n",
    "        model=model,\n",
    "        train_loader=train_data_loader,\n",
    "        optim=optim,\n",
    "        epoch=epoch,\n",
    "        n_epochs=n_epochs,\n",
    "        source_pad_id=pad_index,\n",
    "        target_pad_id=pad_index,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    val_loss, val_losses = validate_epoch(\n",
    "        model=model,\n",
    "        val_loader=valid_data_loader,\n",
    "        epoch=epoch,\n",
    "        n_epochs=n_epochs,\n",
    "        source_pad_id=pad_index,\n",
    "        target_pad_id=pad_index,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {val_loss:7.3f} | Valid PPL: {np.exp(val_loss):7.3f}\")\n",
    "\n",
    "    if val_loss < best_valid_loss:\n",
    "        best_valid_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfFBE8vxqv3j",
    "outputId": "3fca2406-a7aa-479a-8369-45d45eac7438"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "model_save_path = '/content/drive/MyDrive/transformer/my_model_uk-en.pth'\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHMUucKJrC4P"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import log\n",
    "\n",
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    uk_nlp,\n",
    "    en_nlp,\n",
    "    uk_vocab,\n",
    "    en_vocab,\n",
    "    device,\n",
    "    max_output_length=50,\n",
    "    beam_size=3,\n",
    "    lower=False,\n",
    "    sos_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    print_process=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Translate a sentence using the Transformer model with beam search\n",
    "\n",
    "    Args:\n",
    "        sentence: Input sentence to translate (string or list of tokens)\n",
    "        model: Transformer model\n",
    "        uk_nlp: Ukrainian spacy tokenizer\n",
    "        en_nlp: English spacy tokenizer\n",
    "        uk_vocab: Source vocabulary\n",
    "        en_vocab: Target vocabulary\n",
    "        device: torch device\n",
    "        max_output_length: Maximum output length\n",
    "        beam_size: Beam size for beam search\n",
    "        lower: Whether to lowercase tokens\n",
    "        sos_token: Start of sentence token\n",
    "        eos_token: End of sentence token\n",
    "        print_process: Whether to print decoding process\n",
    "\n",
    "    Returns:\n",
    "        translated_tokens: List of translated tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input sentence\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = [token.text for token in uk_nlp.tokenizer(sentence)]\n",
    "    else:\n",
    "        tokens = [token for token in sentence]\n",
    "\n",
    "    if lower:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Add SOS/EOS and convert to indices\n",
    "    tokens = [sos_token] + tokens + [eos_token]\n",
    "    src_ids = torch.LongTensor(uk_vocab.lookup_indices(tokens)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Create source mask\n",
    "    src_mask = model.make_source_mask(src_ids, uk_vocab[eos_token])\n",
    "\n",
    "    # Encode source sentence\n",
    "    with torch.no_grad():\n",
    "        memory = model.encoder(src_ids, src_mask)\n",
    "\n",
    "    # Initialize beams (tokens, log_prob)\n",
    "    beams = [([en_vocab[sos_token]], 0.0)]\n",
    "    completed = []\n",
    "\n",
    "    for step in range(max_output_length):\n",
    "        if not beams:\n",
    "            break\n",
    "\n",
    "        # Prepare batch for all beams\n",
    "        beam_inputs = torch.LongTensor([beam[0] for beam in beams]).to(device)\n",
    "        batch_size = beam_inputs.size(0)\n",
    "\n",
    "        # Create target mask\n",
    "        trg_mask = model.make_target_mask(beam_inputs)\n",
    "\n",
    "        # Expand memory for batch\n",
    "        memory_expanded = memory.expand(batch_size, -1, -1)\n",
    "\n",
    "        # Decode\n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(beam_inputs, memory_expanded, src_mask, trg_mask)\n",
    "            logits = model.final_linear(output[:, -1, :])\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "        # Get top candidates\n",
    "        top_k_scores, top_k_indices = log_probs.topk(beam_size, dim=1)\n",
    "\n",
    "        # Generate new beams\n",
    "        new_beams = []\n",
    "        for i, (tokens, score) in enumerate(beams):\n",
    "            for j in range(beam_size):\n",
    "                new_tokens = tokens + [top_k_indices[i,j].item()]\n",
    "                new_score = score + top_k_scores[i,j].item()\n",
    "                new_beams.append((new_tokens, new_score))\n",
    "\n",
    "        # Select top beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "        # Check for completed beams\n",
    "        new_beams = []\n",
    "        for beam in beams:\n",
    "            tokens, score = beam\n",
    "            last_token = tokens[-1]\n",
    "\n",
    "            if last_token == en_vocab[eos_token] or step == max_output_length-1:\n",
    "                completed.append(beam)\n",
    "            else:\n",
    "                new_beams.append(beam)\n",
    "\n",
    "        beams = new_beams\n",
    "\n",
    "        if not beams:\n",
    "            break\n",
    "\n",
    "    # If no beams completed, use incomplete beams\n",
    "    if not completed:\n",
    "        completed = beams[:1]\n",
    "\n",
    "    # Length normalization (to favor longer sequences)\n",
    "    completed = [\n",
    "        (tokens, score / (len(tokens)**0.7))\n",
    "        for tokens, score in completed\n",
    "    ]\n",
    "\n",
    "    # Select best beam\n",
    "    best_tokens = max(completed, key=lambda x: x[1])[0]\n",
    "\n",
    "    # Convert indices to tokens\n",
    "    translated_tokens = en_vocab.lookup_tokens(best_tokens)\n",
    "\n",
    "    # Remove SOS/EOS\n",
    "    translated_tokens = [t for t in translated_tokens if t not in [sos_token, eos_token]]\n",
    "\n",
    "    return translated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jvq18Rohhzch",
    "outputId": "792085c9-128b-44f3-df7d-f4f333c2907a"
   },
   "outputs": [],
   "source": [
    "sentence = test_data[0][\"uk\"]\n",
    "expected_translation = test_data[0][\"en\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S6tAGAeh-_1"
   },
   "outputs": [],
   "source": [
    "translation = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    uk_nlp,\n",
    "    en_nlp,\n",
    "    uk_vocab,\n",
    "    en_vocab,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jeP9yXRjDSE",
    "outputId": "5a627d5e-c12f-4555-99d5-7c104a7faf5a"
   },
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UD6ehF2-nf9X",
    "outputId": "92ce957d-cc45-4541-f5c3-5a11d9d9fa19"
   },
   "outputs": [],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        example[\"uk\"],  \n",
    "        model,          \n",
    "        uk_nlp,         \n",
    "        en_nlp,         \n",
    "        uk_vocab,       \n",
    "        en_vocab,       \n",
    "        device,         \n",
    "        print_process=False\n",
    "    )\n",
    "    for example in tqdm(test_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WZsJejLkDF8"
   },
   "outputs": [],
   "source": [
    "predictions = [\" \".join(translation[1:-1]) for translation in translations]\n",
    "\n",
    "references = [[example[\"en\"]] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yraagIAekFzy",
    "outputId": "0e6647a4-b69a-49f8-f924-6cf7d1d9fe61"
   },
   "outputs": [],
   "source": [
    "predictions[0], references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJNDVB7qkGLr"
   },
   "outputs": [],
   "source": [
    "def get_tokenizer_fn(nlp, lower):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = [token.text for token in nlp.tokenizer(s)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqUUW4q2kIA0"
   },
   "outputs": [],
   "source": [
    "tokenizer_fn = get_tokenizer_fn(en_nlp, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQG-L9BpkKNA"
   },
   "outputs": [],
   "source": [
    "predictions = [tokenizer_fn(\" \".join(translation[1:-1])) for translation in translations]\n",
    "references = [[tokenizer_fn(example[\"en\"].strip())] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3IssnAm3nPts",
    "outputId": "22d653af-8c2a-4142-b166-0236f6a1107a"
   },
   "outputs": [],
   "source": [
    "len(predictions), len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bFEBYGJwzuby",
    "outputId": "52eb2cdc-e6d1-4c13-f94a-ca236e3a0d7c"
   },
   "outputs": [],
   "source": [
    "print(\"Sample translations:\")\n",
    "for i in range(10):\n",
    "    example = test_data[i]  \n",
    "    source = example[\"uk\"] \n",
    "    ref = example[\"en\"]     \n",
    "    hyp = \" \".join(predictions[i])  \n",
    "\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Source (UK): {source}\")\n",
    "    print(f\"Reference (EN): {ref}\")\n",
    "    print(f\"Translation (EN): {hyp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hmW2sTwQkMMu",
    "outputId": "490e9899-6c42-43b4-9e48-e2b1e43441c3"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "score = bleu_score(\n",
    "    predictions, references\n",
    ")\n",
    "\n",
    "print(f\"BLEU score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
