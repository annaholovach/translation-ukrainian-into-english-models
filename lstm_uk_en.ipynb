{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_hzmNUCOVyI",
    "outputId": "5fa1556d-ffb5-4b3b-d50d-b4fb81fcb712"
   },
   "outputs": [],
   "source": [
    "!pip install torchtext==0.15.1\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461,
     "referenced_widgets": [
      "dd9ed187cfec4457886dff04ac7a4369",
      "a0541b7f8bdb4b7b8a35f752a37201d8",
      "41b084e50ab94c82a5980c723a75cb99",
      "aff39bbb2aff435794944c8c1a50ddb4",
      "bd182e08857a4c6bbe2d8a68ba2b0da5",
      "2b8c0401cac045029822c395a2159605",
      "67e5957209ac4e87863b18c2fbeb1e02",
      "e10fea69af464ea4b94145aaaccc15b7",
      "a0365f907d124902a4e4908e8b97c377",
      "94c3285f721f4fd586f97dcbdb1de1fc",
      "f0299793fbe249bf948d5002b48a8169",
      "07dcc8b029f347b9bd0f42918dfba985",
      "32bfd44170ba44f7b0497905cb501385",
      "55599a250d9649eb8c6df1246883f75a",
      "9975009e187c4adda929aec4e08f5b92",
      "abfd652307e243f1a686bcd104f4c050",
      "bbd2b15c32ed4410b7dc85a9d50605a3",
      "e3123913955844b1be417b667239c816",
      "319e71a90534421e99719c7ab8e14cfa",
      "b6536d5f4aa645768bec32866f0cc635",
      "76ca18d2160041aa83eaed8d959ce48c",
      "7e6f9384dad44bdca5f6db1ab7033b87",
      "98b356299c534d49a11dde4a82437057",
      "2b47471584254d05b0dd489040d12be1",
      "980778f2e9b94cec93293843cd2b4cb4",
      "aa11477cd3e94e108ce6710c9ff3750a",
      "d0cce33a0aa440dfa7775d6cddc3222c",
      "a0a6e0a3288a48a19bae4f5363ada32b",
      "91e17b28fe1f45e68e46f8d4fef6e713",
      "0b3147824ac54c3cb33b88c9f2ce109b",
      "ecdfb8eba2204714847c7410b1e03317",
      "3cb7f56bd16240d1958b7e8a47502f88",
      "7bbeffb7f4684911b05d9848cc7de02e",
      "2d890f2107174620b595d000d9f92576",
      "e9d832c9cefe44db8d5fe9561e7413eb",
      "41dedfbe10194ff2bc8b533cbd5f9add",
      "ec8f2c7355654957b5c7750f147b7294",
      "28aa6a0d812045e2bb52842923b09939",
      "7f644ab1fd0d41068ce0fb491ae97c75",
      "771ac7a903c74cf0ab1aa6f14bb7235c",
      "7ac9d6cab45e4336a09f4b5daf7de3ad",
      "d8af38a9ddb54f3b818b43e155f2024d",
      "e56e219df0964dfa9af478165f685c53",
      "ec8d0065c25a4987b3461710554177c4",
      "f6b92de3212542289cdd647d442958d7",
      "e64d8910eb774a1f8426101400af1e7b",
      "689cb865ef874d499a54fb42edcfe37b",
      "ae805a452e0e46759cbe5af01053b2c9",
      "e000d79551134532a0241fbd8f4e1efc",
      "15e565d7790c4d8eabc7b04f8bc2475c",
      "bc767b9a753746928b8d152b046a48b8",
      "9d0079942a3b43e89c651e230c555f2f",
      "cfd39956df6e43a4a6201b51c1fb773a",
      "7b2901870afe453880bda53a30dc183a",
      "7c07251040f24f60b4411651542ffc1d",
      "76896c2cf4b84cf3b7d23f9499a6425e",
      "9b952cdfe76c4bb2919c9f03e58cc6ca",
      "caa6af88d62f4cb28fef15990d0b7726",
      "393b30e6eba045be99be8a458b82a8c3",
      "20d776ababe94f02acafea3d939c0b88",
      "f7aac263fb88448ebc2b0259313873bd",
      "cb7aa35049c64e598c6b1dfc6419d3f9",
      "76e4318839bc45a68ee1bd25f9dee0f0",
      "51ed616157d94d87b64e357bb2112ab5",
      "b9e0607a6f204808b63d5f5d7e459c75",
      "680b68800a6848a4a5b7fa84bbe4a587",
      "58e1062d9b7a46a2b403e70b7aa730bd",
      "79189e911d9543b4882f8786bad8d699",
      "587e98fe4e0b45878de7a2f6aad76782",
      "67536e58cb0e46f5ac92c631023fc7ef",
      "5d59adc46eb440f5979a1e799e853e57",
      "26e937d00a2341a5a840d1039363bea3",
      "1c92770e03094d7e906e04ccf49a1840",
      "b0ba2cd5f8d3458f876afc704d9a5795",
      "d79bfd967fda458fae55831661c41ee4",
      "51bfc3248e024e268b1a2872e301cf06",
      "5c7750fc56a74d129d93cf0b67d96d76",
      "442d7a5414764a1888ac75d7fe92dc7a",
      "dd19386943084e20811014e7fc6d12c5",
      "633233aa46514ea890d50f9b8f8d9f72",
      "bfe86f54e81f4572bf4f8f58a10fe093",
      "4107223f6db94af1b60b85520a4a4128",
      "10eb296fec604bf2884d19d51c2ef2f8",
      "5199d00e7d2b46dda6837f605990e5d7",
      "2f9814a02c0a4e5e8f9f688442cbea79",
      "8ed81f4d037045249e905a44b879f318",
      "8f50e31c200b46d3a571be7da46aa899",
      "39617a7d78c943ffac9bcf190baa1e2f",
      "6a6d3e071ce44db4890f3377e9655ee6",
      "80276d64f83a47688b50217adc9abcde",
      "c78c1e0f8945432089f8de1dfdc8652d",
      "1ca18162391b4854bf4fb4f2645094b2",
      "f8258b0e603e4446b9ed9b72cae1656f",
      "60345d6e7c7d4967aa366e2869e28ba2",
      "21642657169e4ec5887f2cdde58f4660",
      "63965b5a2273447b889b9eebd19de1de",
      "54c914fb76f34082b8c84608efac31c3",
      "952b938f934e45c08c1d0d5bebab9285",
      "24ed4caa9e704ad1ac82f9e5dec4e5af",
      "6f0f1f32b7884fd5804a58d242b766d3",
      "b6c38d8ae68543ba89b6df609dd05b8f",
      "8d54697e2d044899a256d82e983f4117",
      "c2b492efaae5428388d34189723dc506",
      "77ae69a7820e48e4afc088923390b888",
      "8196c53914fe46ce98bd1109b8403d95",
      "8a9ad8eccbfb4651ad828a5a46558f7a",
      "468e74a98fa64d50b67f22d554c6cdac",
      "ac827f67cdec421d8242b5d694c62251",
      "3818e8286a5847659b3ddb4ff2200f7b",
      "dbc4f30843eb4f6b9a9fb3213bb7db12"
     ]
    },
    "id": "CBQ5BKLaRL0D",
    "outputId": "e4bbeb25-ac32-4184-a3c8-c519342392f7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds2016 = load_dataset(\"turuta/Multi30k-uk\", \"flickr_2016\")\n",
    "ds2017 = load_dataset(\"turuta/Multi30k-uk\", \"flickr_2017\")\n",
    "ds2018 = load_dataset(\"turuta/Multi30k-uk\", \"flickr_2018\")\n",
    "multi = load_dataset(\"turuta/Multi30k-uk\", \"multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQdaZufmSBqo",
    "outputId": "3fe03025-1617-47ae-949a-10028b5c8b1d"
   },
   "outputs": [],
   "source": [
    "print(f'ds2016 {ds2016}')\n",
    "print(f\"ds2017 {ds2017}\")\n",
    "print(f\"ds2018 {ds2018}\")\n",
    "print(f\"multi {multi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7VEZP96hPUop",
    "outputId": "1530f18a-a983-490f-b56e-dae1528e26ac"
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, DatasetDict\n",
    "from sklearn.model_selection import train_test_split as sklearn_train_test_split\n",
    "\n",
    "combined_train = concatenate_datasets([\n",
    "    ds2016[\"train\"],\n",
    "    ds2017[\"train\"],\n",
    "    ds2018[\"train\"],\n",
    "    multi[\"train\"]\n",
    "])\n",
    "\n",
    "train_indices, test_indices = sklearn_train_test_split(\n",
    "    list(range(len(combined_train))),\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = combined_train.select(train_indices)\n",
    "test_dataset = combined_train.select(test_indices)\n",
    "\n",
    "combined_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NW8dtGlUPmVx",
    "outputId": "c21e1541-33e3-44f8-a5d1-8ffe7c54492e"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download uk_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JG650isuPdIw"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "uk_nlp = spacy.load(\"uk_core_news_sm\")\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkQYKPNoTlmu"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [\n",
    "        token for token in tokens\n",
    "        if token.strip() and token.strip() not in {\",\", \".\", \"!\", \"?\", \":\", \";\", \"\\xa0\", \" \"}\n",
    "    ]\n",
    "\n",
    "def tokenize_example(example, uk_nlp, en_nlp, max_length, lower, sos_token, eos_token):\n",
    "    uk_tokens = [token.text for token in uk_nlp.tokenizer(example[\"uk\"])][:max_length]\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    if lower:\n",
    "        uk_tokens = [token.lower() for token in uk_tokens]\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "\n",
    "    uk_tokens = clean_tokens(uk_tokens)\n",
    "    en_tokens = clean_tokens(en_tokens)\n",
    "\n",
    "    uk_tokens = [sos_token] + uk_tokens + [eos_token]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    return {\"uk_tokens\": uk_tokens, \"en_tokens\": en_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "537c5624ed82464b993c3280d70f2433",
      "9846768e979646cfb4547e6e33e52a99",
      "806e8d0b09eb4792b9fb7dffed9f6fde",
      "9a1eca7a693544b1bfb52aa6e1a3c003",
      "b7df193d25f44aba9aa73b0836bdc335",
      "9a19ff7fd38a40c7981339634ce37366",
      "09cc8ecaf08f4b3ba27d72bde072caf3",
      "17c1e657458946b59420fda0b463008d",
      "d3a47006336a4acb92a6aa343713b2b7",
      "a8537e0905684bdb844f671674218a97",
      "a85467873b1b4e5e9a019cb0dab2bcfb",
      "cd7cded4ea6240ac95f5b4f857ad4f6b",
      "f93a55ff380f43e491e46da2ba9e4acf",
      "0ca10a2a54e74252abfea1aa560d4911",
      "77fa6b458c7d4a5c8dab042ad1ec5dab",
      "8f89369879bd4e3391e021273deee438",
      "2d4ac5efd0a04bbcb54be13b1decce9b",
      "19eb7eafe3da412b9b2e83f0e85f4293",
      "502a1cb532f343fc8c764afed5d7ad35",
      "1684b9ea34604e4396506d3fb6a70a0d",
      "695179b4aebc44ea89168ef7fc0443f4",
      "75ccb3a69ad54414a0281f1c5d0108eb"
     ]
    },
    "id": "abOoBEafT8wv",
    "outputId": "ea2cb0e4-1ada-47b8-9d6c-0f255640e052"
   },
   "outputs": [],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"uk_nlp\": uk_nlp,\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_dataset.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sucBoU0EXIPg",
    "outputId": "71b60668-4bc5-45c0-d1a6-2dbcd6bd3fe9"
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dRmY1rNXRnu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "uk_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"uk_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hx8NutWOXuzH",
    "outputId": "99764086-2afe-4331-c6f6-75fcf6efe615"
   },
   "outputs": [],
   "source": [
    "uk_vocab.get_itos()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfSpzNqOX3I2",
    "outputId": "cb7921f7-75fd-4da2-ca2a-a3bf2ab8fb65"
   },
   "outputs": [],
   "source": [
    "en_vocab.get_itos()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIMHsuNbX-Xm",
    "outputId": "d8e6da68-714b-47d2-9815-afd1f7c95006"
   },
   "outputs": [],
   "source": [
    "len(uk_vocab), len(en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEt5bBfdYG_-"
   },
   "outputs": [],
   "source": [
    "assert uk_vocab[unk_token] == en_vocab[unk_token]\n",
    "assert uk_vocab[pad_token] == en_vocab[pad_token]\n",
    "\n",
    "unk_index = uk_vocab[unk_token]\n",
    "pad_index = uk_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2t8b97kYN0e"
   },
   "outputs": [],
   "source": [
    "uk_vocab.set_default_index(unk_index)\n",
    "en_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OljFiacSYcwn",
    "outputId": "feeaa72c-8e40-4e06-f490-961efe1cf74b"
   },
   "outputs": [],
   "source": [
    "tokens = ['a',\n",
    " 'in',\n",
    " 'the',\n",
    " 'on',\n",
    " 'man',\n",
    " 'is',\n",
    " 'and',\n",
    " 'of']\n",
    "en_vocab.lookup_indices(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3k3CB3KYtw8",
    "outputId": "c6f8d924-3010-4fa4-b7ff-4d712c40f644"
   },
   "outputs": [],
   "source": [
    "en_vocab.lookup_tokens(en_vocab.lookup_indices(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEAreOdtY40u"
   },
   "outputs": [],
   "source": [
    "def numericalize_example(example, uk_vocab, en_vocab):\n",
    "    uk_ids = uk_vocab.lookup_indices(example[\"uk_tokens\"])\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    return {\"uk_ids\": uk_ids, \"en_ids\": en_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "51b11ad4e65b4cc2adff26490603df20",
      "46633a2a4bd447638789744cd939981c",
      "5457c8b0e9774aac925463f4efe59b1a",
      "5265ce33bf2043e7ab4e52a761157810",
      "83d5857a624a4ae8a7916a251ddd8812",
      "1891e167d7694fb8a768ddb858592ccc",
      "fb535b4a567f459296d8f97831c3bf62",
      "12eb45fdef184af782fd296fdf90558c",
      "3c659ff9f6ef429f86f3c11fa31aa90f",
      "3a0567e1a9dd4bcc92a4e2930a0be9ff",
      "ea303f84694641949bb221cd80b0fb66",
      "9085f07c779a454f9d990ce94dafe925",
      "55932f0d540a47bc8acfefb7d98d84c6",
      "5a2a2124dc5c4d518dbf3446bcc0b662",
      "0f1206e0aaf54cdb8396fad421085f66",
      "0a58c17643404311915c97a06c7cbbc9",
      "921ea7f2542944feb9bbf12364f3752e",
      "84b6ee6c9d634a27aee359aedc75af55",
      "e52d671c94eb463f8b03884c5b7b0b93",
      "2e9c421c073c4e48b0e3bfcb20e35fa1",
      "710aff0e47954b9abf33491abe042d83",
      "fb7419ba1d064f079f7b95061f3a960c"
     ]
    },
    "id": "2WF2cBvUZC_u",
    "outputId": "97a9de3a-9256-438b-b7a9-5dab3784fbb2"
   },
   "outputs": [],
   "source": [
    "fn_kwargs = {\"uk_vocab\": uk_vocab, \"en_vocab\": en_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDHED-mSZSwv",
    "outputId": "3a58730b-2216-4e59-b3be-ea3b8c29ffaa"
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5U7WtG1UZZJe",
    "outputId": "8e663c35-3d63-49ac-938d-4468a6601aa3"
   },
   "outputs": [],
   "source": [
    "uk_vocab.lookup_tokens(train_data[0][\"uk_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYSllPGVZpJG"
   },
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"uk_ids\", \"en_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDhetwGrZy92",
    "outputId": "57466eb6-b6a5-4754-f27b-18875937340c"
   },
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE_o3fjiZ70w"
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_uk_ids = [example[\"uk_ids\"] for example in batch]\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_uk_ids = nn.utils.rnn.pad_sequence(batch_uk_ids, padding_value=pad_index)\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"uk_ids\": batch_uk_ids,\n",
    "            \"en_ids\": batch_en_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REK22UmgaM2-"
   },
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvVB7We_aPMF"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrAVB8O8aSfk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # outputs are always from the top hidden layer\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNWpkUZTal5F"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch size]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # n directions in the decoder will both always be 1, therefore:\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # context = [n layers, batch size, hidden dim]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [seq length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # seq length and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, hidden dim]\n",
    "        # hidden = [n layers, batch size, hidden dim]\n",
    "        # cell = [n layers, batch size, hidden dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKDT9H2zapJG"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "        ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # cell = [n layers * n directions, batch size, hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        # input = [batch size]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, hidden dim]\n",
    "            # cell = [n layers, batch size, hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mg_-LfKAatzW"
   },
   "outputs": [],
   "source": [
    "input_dim = len(uk_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A932Nh4_bRO1",
    "outputId": "c3f2187b-a2b4-4fec-8288-2b64d5bd7441"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9w0NXUBbWUt",
    "outputId": "52722a4d-cb5c-4ff6-e9fc-3bba89f9fd7b"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJStDL-ebaEt"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wv4NmrLbfEu"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suOCOpJqbmJe"
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"uk_ids\"].to(device)\n",
    "        trg = batch[\"en_ids\"].to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BUVfh0a-b3ru",
    "outputId": "fb792f1a-c11a-4400-9c38-63562dbfdea5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "n_epochs = 20\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_KZY3tMBcmW",
    "outputId": "fb5906f5-676e-4ccf-a1ee-bfe33a27c990"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MX0DJCLbBrf7"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_save_path = '/content/drive/MyDrive/lstm/my_model_uk-en.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "vocab_save_path = '/content/drive/MyDrive/lstm/vocab.json'\n",
    "\n",
    "with open(vocab_save_path, 'w') as f:\n",
    "    json.dump({\n",
    "        \"uk_vocab\": uk_vocab.get_stoi(),\n",
    "        \"en_vocab\": en_vocab.get_stoi()\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLovq5rJ34E_"
   },
   "source": [
    "# Новий розділ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0QMo-t434iy"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    uk_nlp,\n",
    "    en_nlp,\n",
    "    uk_vocab,\n",
    "    en_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in uk_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = uk_vocab.lookup_indices(tokens)\n",
    "\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "\n",
    "        inputs = en_vocab.lookup_indices([sos_token])\n",
    "\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "\n",
    "            if predicted_token == en_vocab[eos_token]:\n",
    "                break\n",
    "\n",
    "        tokens = en_vocab.lookup_tokens(inputs)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5iQtzdeQ4Q6j",
    "outputId": "1edcef20-e933-4e59-f25f-4af9452b257f"
   },
   "outputs": [],
   "source": [
    "sentence = test_data[0][\"uk\"]\n",
    "expected_translation = test_data[0][\"en\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRDxtIfk4UbS"
   },
   "outputs": [],
   "source": [
    "translation = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    uk_nlp,\n",
    "    en_nlp,\n",
    "    uk_vocab,\n",
    "    en_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5y2HQEM44xMq",
    "outputId": "0e51662d-4834-4fe0-e658-6c847a6e1df9"
   },
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IYtJHRkw45TL",
    "outputId": "79d1db7a-6681-49d5-d000-56f935b430fe"
   },
   "outputs": [],
   "source": [
    "translations = [\n",
    "    translate_sentence(\n",
    "        example[\"uk\"],\n",
    "        model,\n",
    "        uk_nlp,\n",
    "        en_nlp,\n",
    "        uk_vocab,\n",
    "        en_vocab,\n",
    "        lower,\n",
    "        sos_token,\n",
    "        eos_token,\n",
    "        device,\n",
    "    )\n",
    "    for example in tqdm(test_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nqkQTVD5Gx8"
   },
   "outputs": [],
   "source": [
    "predictions = [\" \".join(translation[1:-1]) for translation in translations]\n",
    "\n",
    "references = [[example[\"en\"]] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PGxaSXI15Hc5",
    "outputId": "181b02de-08b0-416e-f1e3-49e7eac8bcdf"
   },
   "outputs": [],
   "source": [
    "predictions[0], references[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1BgXaKj5TAG"
   },
   "outputs": [],
   "source": [
    "def get_tokenizer_fn(nlp, lower):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = [token.text for token in nlp.tokenizer(s)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "\n",
    "    return tokenizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHN_Jzkm5UAJ"
   },
   "outputs": [],
   "source": [
    "tokenizer_fn = get_tokenizer_fn(en_nlp, lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LVzHD2E4-Rw"
   },
   "outputs": [],
   "source": [
    "predictions = [tokenizer_fn(\" \".join(translation[1:-1])) for translation in translations]\n",
    "references = [[tokenizer_fn(example[\"en\"].strip())] for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11QNzHPS59wA",
    "outputId": "46724acb-72bc-4503-8724-f09a823f5826"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "score = bleu_score(\n",
    "    predictions, references\n",
    ")\n",
    "\n",
    "print(f\"BLEU score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
